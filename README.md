# Asymptotic Generalization Error in the Online Learning of Linearized Two-Layer Neural Networks
Submitted to ML4PS <nb>
## Abstract <nb>
Deep neural networks are widely used prediction algorithms whose performance often improves as the number of weights increases, leading to over-parametrization. Under a proper rescaling of the network output, such networks can be linearized in their training weights. Here, we consider linearized two-layered neural networks in an over-parametrized setting and study their generalizability in the context of a student-teacher framework. For any finite ratio of hidden layer size and input dimension, the student cannot generalize perfectly, and we compute the non-zero asymptotic generalization error.
